# Anime-Face-GAN

This implementation employs PyTorch’s `Dataset`/`DataLoader` abstractions to ingest and preprocess over 63 000 raw anime face images: each image is center-cropped to a square, resized to 64×64 px, and normalized to the \[–1, 1] range in a single, reproducible pipeline. The core GAN follows the DCGAN (Deep Convolutional GAN) paradigm, with the generator constructed from a series of transposed-convolutional blocks (each paired with batch-normalization and configurable ReLU→Tanh activations) and the discriminator built from strided-convolutional blocks (with batch-normalization and LeakyReLU→Sigmoid). Both networks are defined in standalone modules so that architectural parameters (feature-map widths, activation choices, latent-vector dimensionality) can be swapped via command-line flags without touching the training logic.

Training alternates between binary-cross-entropy updates to the discriminator and generator, using the Adam optimizer (β₁ = 0.5, β₂ = 0.999). A systematic hyperparameter sweep, varying learning rates (1e-4 to 3e-4), batch sizes (64–256), latent dimensions (64–256), and activation functions, identified a stable regime that avoids mode collapse. The `train.py` script (and accompanying Jupyter notebook) logs losses via tqdm (with optional TensorBoard callbacks), checkpoints model weights every N epochs into a `models/` directory, and exports 8×8 sample grids into `generated/` for rapid visual inspection. CUDA support is enabled by default, ensuring scalable GPU training and seamless pause/resume through saved checkpoints.
